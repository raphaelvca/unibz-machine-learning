{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 05 Deep Learning with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20 and TensorFlow ≥2.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "LECTURE_ID = \"05\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", LECTURE_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an Image Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's import TensorFlow and Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.4-tf'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercise: Train a deep MLP on the MNIST dataset (you can load it using `keras.datasets.mnist.load_data()`. See if you can get over 98% precision. Try adding all the bells and whistles—save checkpoints, use early stopping, and plot learning curves using TensorBoard.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps to carry out:\n",
    "- load the dataset \n",
    "- explore dataset and normalise it\n",
    "- define the NN model\n",
    "- compile the model\n",
    "- evaluate the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 6s 1us/step\n"
     ]
    }
   ],
   "source": [
    "mnist = keras.datasets.mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.shape\n",
    "y_train_full.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation set & Normalisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "X_test = X_test / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "# keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "model.add(keras.layers.Dense(300, activation=\"relu\",kernel_initializer=\"he_normal\"))\n",
    "model.add(keras.layers.Dense(100, activation=\"relu\",kernel_initializer=\"he_normal\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=0.01),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/30\n",
      "55000/55000 [==============================] - 12s 221us/sample - loss: 0.5830 - accuracy: 0.8439 - val_loss: 0.2978 - val_accuracy: 0.9174\n",
      "Epoch 2/30\n",
      "55000/55000 [==============================] - 11s 192us/sample - loss: 0.2820 - accuracy: 0.9183 - val_loss: 0.2357 - val_accuracy: 0.9342\n",
      "Epoch 3/30\n",
      "55000/55000 [==============================] - 9s 172us/sample - loss: 0.2307 - accuracy: 0.9346 - val_loss: 0.2018 - val_accuracy: 0.9446\n",
      "Epoch 4/30\n",
      "55000/55000 [==============================] - 9s 170us/sample - loss: 0.1972 - accuracy: 0.9443 - val_loss: 0.1791 - val_accuracy: 0.9490\n",
      "Epoch 5/30\n",
      "55000/55000 [==============================] - 10s 185us/sample - loss: 0.1733 - accuracy: 0.9507 - val_loss: 0.1554 - val_accuracy: 0.9572\n",
      "Epoch 6/30\n",
      "55000/55000 [==============================] - 10s 185us/sample - loss: 0.1544 - accuracy: 0.9566 - val_loss: 0.1443 - val_accuracy: 0.9586\n",
      "Epoch 7/30\n",
      "55000/55000 [==============================] - 10s 180us/sample - loss: 0.1388 - accuracy: 0.9610 - val_loss: 0.1329 - val_accuracy: 0.9642\n",
      "Epoch 8/30\n",
      "55000/55000 [==============================] - 10s 180us/sample - loss: 0.1255 - accuracy: 0.9646 - val_loss: 0.1220 - val_accuracy: 0.9660\n",
      "Epoch 9/30\n",
      "55000/55000 [==============================] - 10s 180us/sample - loss: 0.1146 - accuracy: 0.9681 - val_loss: 0.1162 - val_accuracy: 0.9674\n",
      "Epoch 10/30\n",
      "55000/55000 [==============================] - 10s 184us/sample - loss: 0.1049 - accuracy: 0.9711 - val_loss: 0.1080 - val_accuracy: 0.9708\n",
      "Epoch 11/30\n",
      "55000/55000 [==============================] - 10s 187us/sample - loss: 0.0970 - accuracy: 0.9728 - val_loss: 0.1036 - val_accuracy: 0.9708\n",
      "Epoch 12/30\n",
      "55000/55000 [==============================] - 11s 198us/sample - loss: 0.0892 - accuracy: 0.9754 - val_loss: 0.0996 - val_accuracy: 0.9716\n",
      "Epoch 13/30\n",
      "55000/55000 [==============================] - 10s 184us/sample - loss: 0.0829 - accuracy: 0.9774 - val_loss: 0.0970 - val_accuracy: 0.9712\n",
      "Epoch 14/30\n",
      "55000/55000 [==============================] - 15s 268us/sample - loss: 0.0771 - accuracy: 0.9788 - val_loss: 0.0896 - val_accuracy: 0.9752\n",
      "Epoch 15/30\n",
      "55000/55000 [==============================] - 17s 302us/sample - loss: 0.0721 - accuracy: 0.9803 - val_loss: 0.0869 - val_accuracy: 0.9756\n",
      "Epoch 16/30\n",
      "55000/55000 [==============================] - 15s 270us/sample - loss: 0.0668 - accuracy: 0.9823 - val_loss: 0.0864 - val_accuracy: 0.9774\n",
      "Epoch 17/30\n",
      "55000/55000 [==============================] - 12s 227us/sample - loss: 0.0625 - accuracy: 0.9831 - val_loss: 0.0820 - val_accuracy: 0.9784\n",
      "Epoch 18/30\n",
      "55000/55000 [==============================] - 10s 185us/sample - loss: 0.0587 - accuracy: 0.9840 - val_loss: 0.0802 - val_accuracy: 0.9774\n",
      "Epoch 19/30\n",
      "55000/55000 [==============================] - 9s 164us/sample - loss: 0.0551 - accuracy: 0.9853 - val_loss: 0.0778 - val_accuracy: 0.9784\n",
      "Epoch 20/30\n",
      "55000/55000 [==============================] - 11s 194us/sample - loss: 0.0517 - accuracy: 0.9862 - val_loss: 0.0766 - val_accuracy: 0.9788\n",
      "Epoch 21/30\n",
      "55000/55000 [==============================] - 10s 190us/sample - loss: 0.0486 - accuracy: 0.9876 - val_loss: 0.0744 - val_accuracy: 0.9800\n",
      "Epoch 22/30\n",
      "55000/55000 [==============================] - 10s 184us/sample - loss: 0.0455 - accuracy: 0.9885 - val_loss: 0.0747 - val_accuracy: 0.9790\n",
      "Epoch 23/30\n",
      "55000/55000 [==============================] - 11s 208us/sample - loss: 0.0429 - accuracy: 0.9892 - val_loss: 0.0743 - val_accuracy: 0.9778\n",
      "Epoch 24/30\n",
      "55000/55000 [==============================] - 12s 225us/sample - loss: 0.0405 - accuracy: 0.9897 - val_loss: 0.0740 - val_accuracy: 0.9786\n",
      "Epoch 25/30\n",
      "55000/55000 [==============================] - 10s 187us/sample - loss: 0.0380 - accuracy: 0.9906 - val_loss: 0.0710 - val_accuracy: 0.9808\n",
      "Epoch 26/30\n",
      "55000/55000 [==============================] - 10s 175us/sample - loss: 0.0360 - accuracy: 0.9912 - val_loss: 0.0708 - val_accuracy: 0.9800\n",
      "Epoch 27/30\n",
      "55000/55000 [==============================] - 10s 173us/sample - loss: 0.0339 - accuracy: 0.9917 - val_loss: 0.0699 - val_accuracy: 0.9794\n",
      "Epoch 28/30\n",
      "55000/55000 [==============================] - 10s 178us/sample - loss: 0.0319 - accuracy: 0.9928 - val_loss: 0.0694 - val_accuracy: 0.9798\n",
      "Epoch 29/30\n",
      "55000/55000 [==============================] - 11s 191us/sample - loss: 0.0302 - accuracy: 0.9934 - val_loss: 0.0696 - val_accuracy: 0.9810\n",
      "Epoch 30/30\n",
      "55000/55000 [==============================] - 10s 178us/sample - loss: 0.0283 - accuracy: 0.9939 - val_loss: 0.0674 - val_accuracy: 0.9802\n"
     ]
    }
   ],
   "source": [
    "run_index = 1\n",
    "run_logdir = os.path.join(os.curdir, 'myminst_logs','run_{:03d}'.format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "history = model.fit(X_train, y_train, epochs=30,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir=./myminst_logs --port=6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 91us/sample - loss: 0.0746 - accuracy: 0.9773\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07457246828032657, 0.9773]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "nav_menu": {
   "height": "264px",
   "width": "369px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
